{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import yaml\n",
    "import re\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Google Cloud Language Translation API\n",
    "# We're using the basic version here == \"v2\" \n",
    "from google.cloud import translate_v2\n",
    "\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This was amazingly helpful https://www.youtube.com/watch?v=YapTts_An9A \n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r'tt-translate-2023396507e3.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create function for performing the translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_translate_messages(one_row):\n",
    "    '''\n",
    "    Pass in a df row.\n",
    "    Find the message under the 'content column'\n",
    "    '''\n",
    "    one_message = one_row.loc['content']\n",
    "    \n",
    "    # initialize the Google Cloud translation client\n",
    "    translate_client = translate_v2.Client()\n",
    "    \n",
    "    # set the target language\n",
    "    target = 'en'\n",
    "    \n",
    "    \n",
    "    try:\n",
    "    # apply the translation \n",
    "        output = translate_client.translate(one_message, \n",
    "                                            format_='html',\n",
    "                                            target_language=target)\n",
    "    except:\n",
    "        output = {'translatedText': 'translation_error', \n",
    "                  'detectedSourceLanguage': 'translation_error', \n",
    "                  'orig_content':'translation_error'}\n",
    "\n",
    "    return list(output.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prepare dataset for translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All parent-school messages, output from Script 06_no_ra_status_osse_merge\n",
    "all_msgs = pd.read_pickle('../data/analysis_data/messages_w_demographics_osse6_schools_pickle.pkl')\n",
    "all_msgs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Create id for deduplicated content (without removing personalization) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an id for de-duplicated content with no personalization\n",
    "new_id_creation = all_msgs[['content']].drop_duplicates()\n",
    "new_id_creation.shape\n",
    "\n",
    "\n",
    "new_id_creation['id_content_deduped_no_personalization'] = \\\n",
    "    [\"id_\" + str(i) for i in np.arange(1, new_id_creation.shape[0]+1).tolist()]\n",
    "\n",
    "all_msgs_new_id = pd.merge(all_msgs, \n",
    "                           new_id_creation, \n",
    "                           on = \"content\",\n",
    "                           how = \"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Cut down the number of characters we'll run through the translation function\n",
    "\n",
    "We care about this because the Google Cloud pricing structure is the same for language detection and language translation itself. Since language translation will return the language detected, we'd only want to run through it once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count N chars in text messages. We care about this bc Google Cloud pricing structure is based on N chars\n",
    "all_msgs_new_id['content_len'] = all_msgs_new_id.content.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduped_msgs = all_msgs_new_id[['id_content_deduped_no_personalization', \n",
    "                                'content', 'content_len']].drop_duplicates()\n",
    "\n",
    "deduped_msgs.shape\n",
    "\n",
    "# len of characters \n",
    "# 16.5million characters might be too much, so we'll cut it down\n",
    "deduped_msgs.content_len.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common phrases to look for in our text message to cut down on messages to translate\n",
    "\n",
    "common_phrases = ['no problem', 'No problem',\n",
    "                  'Hello', 'thank you', 'Thank you', 'Thank You', 'Thanks', 'thanks',\n",
    "                  \"You're welcome\", \"You are very welcome\",\n",
    "                  'English teacher', \n",
    "                  'attendance', \n",
    "                  'was absent from',\n",
    "                  'was not in class',\n",
    "                  'was late to',\n",
    "                  'I will let you know',\n",
    "                  'Please reply to this message',\n",
    "                  'Good Afternoon', 'Good afternoon', 'good afternoon', \n",
    "                  'Good Morning', 'Good morning', 'good morning',\n",
    "                  'Please provide', 'Please respond', 'please check out', 'please contact me',\n",
    "                  'assignment',\n",
    "                  'detention', 'suspended',\n",
    "                  'Good Evening', 'Good evening',\n",
    "                  'Please make sure', 'Please be sure', 'Please send',\n",
    "                  'Please Join',\n",
    "                  \"That's great\",\n",
    "                  'hall sweep', 'The message was',\n",
    "                  'Parent teacher', 'parent teacher', 'parent-teacher',\n",
    "                  'parent/teacher', 'parent/teacher',\n",
    "                  'conferences', 'Conferences', \"That's awesome\",\n",
    "                  'failing', 'fail',\n",
    "                  'I am sending this message to inform ',\n",
    "                  'This message is from',\n",
    "                  'I want to say thank you for your support this first week of distance-learning',\n",
    "                  'did not turn in', \n",
    "                  'First, I would like to say we made it','Early Release Day','Uber','Idgaf', 'presentation',\n",
    "                  'Let us have a great week',\n",
    "                  'Wishing you', \n",
    "                  'Dear Parent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the common phrases\n",
    "# If phrase in content, give the message a 0, else, leave blank\n",
    "deduped_msgs['non_english'] = np.where(deduped_msgs.content.str.contains(('|').join(common_phrases)),\n",
    "                                       0, \n",
    "                                       '')\n",
    "\n",
    "deduped_msgs.shape\n",
    "\n",
    "deduped_msgs.non_english.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is one parent that uses Spanish primarily but includes \"Thanks\" in text, so false positive\n",
    "# Manually add this back by looking for the id\n",
    "deduped_msgs_parent_check = pd.merge(deduped_msgs, \n",
    "                                     all_msgs_new_id[['id_content_deduped_no_personalization',\n",
    "                                                      'StudentID', \n",
    "                                                      'broad_type']], \n",
    "                                    how = 'left', \n",
    "                                    on = 'id_content_deduped_no_personalization')\n",
    "\n",
    "deduped_msgs_parent_check['non_english'] = \\\n",
    "    np.where((deduped_msgs_parent_check.StudentID == 9209061) & \\\n",
    "             (deduped_msgs_parent_check.broad_type == 'parent_sent'), \n",
    "                                                    '', \n",
    "                                                    deduped_msgs_parent_check.non_english)\n",
    "\n",
    "deduped_msgs_parent_check.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrong N rows , so need to go through the process of dropping things again\n",
    "deduped_msgs_rm_enes = deduped_msgs_parent_check[['id_content_deduped_no_personalization', \n",
    "                                                  'content', 'content_len', 'non_english']]\\\n",
    "                       .drop_duplicates()\n",
    "\n",
    "deduped_msgs_rm_enes.shape\n",
    "\n",
    "deduped_msgs_rm_enes.non_english.value_counts()\n",
    "\n",
    "print('N chars to run through translator:', \n",
    "      deduped_msgs_rm_enes[deduped_msgs_rm_enes.non_english != '0'].content_len.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Run the messages that did not have common phrases above through the translation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset df to the ones that are non_english. English == 0\n",
    "df_to_translate = deduped_msgs_rm_enes[deduped_msgs_rm_enes.non_english != '0'].copy()\n",
    "df_to_translate.shape\n",
    "\n",
    "# Split df into 20 dataframes, so that we can revisit if code breaks + internet crashes \n",
    "split_df = np.array_split(df_to_translate, 20)\n",
    "\n",
    "# N rows/columns per df. \n",
    "for i in range(len(split_df)):\n",
    "    print('df', i+1, ':', \n",
    "        split_df[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up to run translation\n",
    "\n",
    "# output path\n",
    "path = '../data/gcloud_translation_results/'\n",
    "filename = 'translated_msgs_'\n",
    "ext = '.pkl'\n",
    "\n",
    "\n",
    "# For every dataframe in split_df, \n",
    "# run through the translation, unpack the results, \n",
    "# and save as pickles\n",
    "\n",
    "for i in range(len(split_df)):\n",
    "    one_df = split_df[i]\n",
    "    \n",
    "    start_translation_time = timeit.default_timer() #time start\n",
    "    \n",
    "    # run translation\n",
    "    one_df['output_list'] = one_df.apply(google_translate_messages, axis = 1)\n",
    "\n",
    "    stop_translation_time = timeit.default_timer() #time end\n",
    "    \n",
    "    time_lapse = stop_translation_time - start_translation_time\n",
    "    print(\"took \" + str(time_lapse) + \" seconds to run\")\n",
    "    \n",
    "    # unpack the translation results into their own columns\n",
    "    one_df[['translatedText', 'detectedSourceLanguage', 'orig_content']] = \\\n",
    "        pd.DataFrame(one_df.output_list.to_list(),  \n",
    "                     index = one_df.index)\n",
    "\n",
    "    one_df.to_pickle(path + filename + str(i) + ext)\n",
    "    \n",
    "    print(\"wrote results for df \", i + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Combine msgs back together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Read in the translated pickles and stitch back together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output path\n",
    "path = '../data/gcloud_translation_results/'\n",
    "filename = 'translated_msgs_'\n",
    "ext = '.pkl'\n",
    "\n",
    "# init first df\n",
    "translated_msgs_init = pd.read_pickle(path + filename + '0' + ext)\n",
    "print(translated_msgs_init.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize list to store our pickles\n",
    "pickles = []\n",
    "\n",
    "# For every file in the folder, read it in as a dataframe, \n",
    "# then append to the pickles list\n",
    "for i in range(1, len(split_df)):\n",
    "    df = pd.read_pickle(path + filename + str(i) + ext)\n",
    "    print(df.shape)\n",
    "    pickles.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append all the dataframes back together\n",
    "translated_msgs = pd.concat([translated_msgs_init, *pickles])\n",
    "\n",
    "print('Does the shape of the new dataframe match the one pre-translation? ')\n",
    "translated_msgs.shape[0] == df_to_translate.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the languages detected\n",
    "translated_msgs.detectedSourceLanguage.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_msgs.detectedSourceLanguage.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore a couple of messages\n",
    "translated_msgs[translated_msgs.detectedSourceLanguage == 'es'].sample(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Add the translated text back to the non-translated deduped messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the english ones in our original based on the common phrases\n",
    "deduped_msgs_en = deduped_msgs_rm_enes[deduped_msgs_rm_enes.non_english=='0'].copy()\n",
    "\n",
    "\n",
    "deduped_msgs_w_translation = deduped_msgs_en.append(translated_msgs, ignore_index = True)\n",
    "\n",
    "# In the old script (non-Google Cloud API), if we translated an English text with incorrect spelling, the \n",
    "# translation would do spell-check/correct. Looks like this isn't the case here\n",
    "deduped_msgs_w_translation[(deduped_msgs_w_translation.detectedSourceLanguage == 'en') &\n",
    "                           (deduped_msgs_w_translation.orig_content != deduped_msgs_w_translation.translatedText)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Clean up deduped messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the columns we don't need\n",
    "deduped_msgs_w_translation.drop(columns = ['content_len', 'output_list', 'orig_content'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html2text\n",
    "\n",
    "def fix_html(one_row):\n",
    "    '''\n",
    "    This function takes in a row and removes the \n",
    "    html tags from the translated messages.\n",
    "    \n",
    "    Example input:  Hello, I am Ms. Johnson. Vicky&#39;s teacher\n",
    "    Example output: Hello, I am Ms. Johnson. Vicky's teacher\n",
    "    '''\n",
    "    \n",
    "    message = one_row.loc['translatedText']\n",
    "    \n",
    "    html_conv = html2text.HTML2Text()\n",
    "    \n",
    "    try:\n",
    "        converted_msg = html_conv.handle(message)\n",
    "    except:\n",
    "        # if errors, e.g. with phone numbers, just go with the original message\n",
    "        converted_msg = message\n",
    "    \n",
    "    return converted_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to remove the tags\n",
    "deduped_msgs_w_translation['translatedText_rm_html_init'] = deduped_msgs_w_translation.apply(fix_html, axis = 1)\n",
    "\n",
    "# the function adds '\\n\\n' so remove thatfrom the text\n",
    "deduped_msgs_w_translation['translatedText_rm_html'] = deduped_msgs_w_translation.translatedText_rm_html_init\\\n",
    "                                                       .str.strip('\\n\\n')\n",
    "\n",
    "# check results\n",
    "deduped_msgs_w_translation[~deduped_msgs_w_translation.translatedText.isna()].head()\n",
    "\n",
    "deduped_msgs_w_translation.drop(columns= ['translatedText_rm_html_init'],\n",
    "                                inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag for whether the message was translated or not \n",
    "# for the english ones that were run through the translator, we'll keep that as no.\n",
    "deduped_msgs_w_translation['translated'] = np.where((~deduped_msgs_w_translation.translatedText.isna()) &\\\n",
    "                                                    (deduped_msgs_w_translation.detectedSourceLanguage != 'en'),\n",
    "                                                    1, 0\n",
    "                                                    )\n",
    "\n",
    "# new column with either the original content or the translated content\n",
    "deduped_msgs_w_translation['content_w_translation'] = np.where(deduped_msgs_w_translation.translated == 0, \n",
    "                                                               deduped_msgs_w_translation.content, \n",
    "                                                               deduped_msgs_w_translation.translatedText_rm_html)\n",
    "\n",
    "\n",
    "deduped_msgs_w_translation['run_thru_translate'] = np.where(deduped_msgs_w_translation.non_english == '0', \n",
    "                                                            0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduped_msgs_w_translation[deduped_msgs_w_translation.translated==1].sample(n=10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Add back to the original set of messages w demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_msgs_new_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_msgs_new_id.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_msgs_wtranslation = pd.merge(all_msgs_new_id, \n",
    "                                 deduped_msgs_w_translation.drop(columns = ['content', 'non_english']), \n",
    "                                 how = 'left', \n",
    "                                 on = 'id_content_deduped_no_personalization')\n",
    "\n",
    "all_msgs_wtranslation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_msgs_wtranslation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_msgs_wtranslation.to_pickle('../data/analysis_data/msgs_wdem_wtrans_1124.pkl')\n",
    "\n",
    "all_msgs_wtranslation.to_pickle('../data/analysis_data/msgs_wdem_wtrans_1203.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
